title = "Arranging 100 Symbols"
published = "2012-09-05T00:00:00.00"
certified = True
^L
^L
{% extends base.html %}
{% block content %}

<p><em>Last night I had my first experience <a
href="/courses/web-development-101/">teaching programming</a> to beginners.
The students had read the first three sections of the HTML 4 specification, and
during the class session we installed a text editor and web browser, and
started building web pages from scratch. Based on the questions they asked and
the discussion we had, I decided that it might help to zoom out a level and
frame the concept of &ldquo;programming&rdquo; in general terms. This is what I
came up with.</em></p>

<hr />

<p>Computers are a tool to extend our power of reason.</p>

<p>Reason can be thought of as a processing of symbols according to rules.</p>

<p>Programming consists in arranging symbols according to rules, for the
purpose of being processed according to other rules.</p>

<p>Several sets of symbols have evolved in programming. By far the most
important is the <a href="https://en.wikipedia.org/wiki/ASCII">set of
symbols</a> known as &ldquo;ASCII&rdquo; (pronounced <i>ass-key</i>). There are
128 symbols in ASCII. The obvious ones are the letters of the English alphabet,
lowercase and uppercase (a-z, A-Z) as well as the digits (0-9). That&rsquo;s 26
+ 26 + 10 = 62 out of 128 symbols, leaving 66 more. Of these remaining, half
(33) are punctuation marks. Together, these 62 + 33 = 95 are called
&ldquo;printable characters,&rdquo; because they are also characters in written
English. The remaining  128 - 95 = 33 are called &ldquo;control
characters.&rdquo; They don&rsquo;t correspond to anything in written English,
but instead have arcane meanings tied to the way early computers were built.
The three most important control characters are:</p>

<table>
<tr><td>\n</td><td>newline</td><td></td></tr>
<tr><td>\r</td><td>carriage return</td><td>think typewriters, if you&rsquo;re old enough</td></tr>
<tr><td>\t</td><td>horizontal tab</td><td>usually just called &ldquo;tab&rdquo;</td></tr>
</table>

<p>Between the printable characters and the few control characters you&rsquo;ll
run across, let&rsquo;s round up and say there are 100 symbols from ASCII that
you&rsquo;ll use in practice as a programmer.</p>

<p>In its very essence, a digital computer works with just two symbols: 0 and 1.
This simplest set of symbols is called &ldquo;binary.&rdquo; The ASCII set of
symbols (also called a &ldquo;character set&rdquo;), was invented in the 1960s
as a more convenient way for humans to work with computers. The computer
translates every ASCII character into binary in order to process it. For
example, the uppercase &ldquo;A&rdquo; character in ASCII becomes 01000001 in
binary. The binary number 01000001 is equal to the decimal number 65: the
&ldquo;A&rdquo; character is 65th out of 128 in the ASCII character set. When
an &ldquo;A&rdquo; character is stored in your computer&rsquo;s memory, that
means that there are eight microscopic pieces of silicon on a microchip inside
your computer, the second and eighth pieces of which have electrons deposited
in them, while the rest are emptied of electrons. You can actually estimate the
number of electrons needed to encode all of the web pages, email, images, etc.
on the Internet, and compute the [mass of the
Internet](http://discovermagazine.com/2007/jun/how-much-does-the-internet-weigh/).
It&rsquo;s, like, millionths of an ounce. I happen to think that that&rsquo;s
pretty wild.</p>

<p>Anyway, your job as a programmer is simply **to arrange the 100 commonly
used ASCII symbols according to your liking**. All programming languages are
sets of rules for arranging these 100 symbols. Even at the very lowest level of
software, one step above hardware, you&rsquo;re still arranging ASCII
characters, albeit in configurations that correspond directly to the <a
href="http://en.wikipedia.org/wiki/X86_instruction_listings">80+ operations</a>
that are hard-wired into your computer&rsquo;s main microchip. Not since the
1970s has it been generally possible to directly manipulate the 0s and 1s in a
computer, and even then it was with toggle switches&mdash;hardware, not
software. To arrange the 100 symbols, you use a type of software program called
a &ldquo;text editor.&rdquo;</p>

<p>The various rules for arranging the 100 symbols are simply conventions that
humans have invented. They are defined <i>de jure</i> in standards, like the <a
href="http://www.w3.org/TR/html401/">HTML 4</a> specification, and <i>de
facto</i> in the hardware and software that do things with the arrangements,
like the <a href="https://www.google.com/intl/en/chrome/browser/">Chrome</a>
web browser.  The promise of Chrome is that if you arrange the 100 symbols
according to the rules of HTML, it will make web pages happen. For example, if
you feed the 22 symbols &ldquo;&lt;h1&gt;Hello, world!&lt;/h1&gt;&rdquo; to
your web browser, it will give you this:</p>

<div style="margin: 24pt 0;">
    <h1>Hello, world!</h1>
</div>

<p>Learning to program means learning the rules, both <i>de jure</i> and <i>de
facto</i>, and training your fingers to use your text editor. That&rsquo;s it!
:-)</p>

<p>The important part is the phrase, &ldquo;according to your liking.&rdquo;
Learning the rules and training your fingers is teleologically completed when
you use your skills to deliver an experience that you&rsquo;ve decided you want
to deliver. The motivation to learn the rules and train your fingers is to
deliver the experience. Design from the outside in, build from the inside out.
Define the experience, and then learn to do, and then do, whatever it takes to
deliver the experience. That&rsquo;s the hacker way.</p>

<hr />

<p><b>P.S.</b> The ASCII character set has a large number of cousins
corresponding roughly to the other human languages besides English. There is a
standard called <a href="http://unicode.org/">Unicode</a> that unifies all of
the world&rsquo;s languages into a single set of symbols. The first 128 symbols
in Unicode are ... the ASCII symbols. Software source code is practically
always written in pure ASCII (it&rsquo;s &ldquo;user input&rdquo; that you need
to handle as Unicode). Learning ASCII is a foundation for Unicode and not a
waste of time.</p>

<p><b>P.P.S.</b> The ASCII control characters are implicated in two programming
headaches. If you hit the &ldquo;Enter&rdquo; key on a Mac, you get \n. On a
Windows computer, you get \r\n. Both simply display a line break, and having to
deal with the discrepancy is one of the occasional headaches of programming.
Another is that, while it&rsquo;s universally considered good style to indent
different parts of your software source code in order to make it more legible,
it&rsquo;s an annoying matter of taste which character to use: \t characters
from your &ldquo;Tab&rdquo; key, or space characters from your spacebar.</p>

{% end %}
